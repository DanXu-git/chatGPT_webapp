{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# chatGPT webapp\n",
    "\n",
    "> Build a local version of chatGPT with openAI API, python gradio.\n",
    "> Which people in a internal internet can use chatGPT without a VPN.\n",
    "> But this will charge you by a foreign credit card with a $18 free try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp chatGPT_webapp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import os, openai, fire\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# set openAI API key from your environment varibale $OPENAI_API_KEY\n",
    "api_key = os.environ.get('OPENAI_API_KEY')\n",
    "openai.api_key = api_key\n",
    "\n",
    "def generate_response(model_engine, messages):\n",
    "    \"Deliver the messages to openAI and get a response\"\n",
    "    model_engine = model_engine\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model_engine,\n",
    "        messages=messages\n",
    "    )\n",
    "    role, content = response['choices'][0]['message']['role'], response['choices'][0]['message']['content']\n",
    "    messages.append({\"role\": role, \"content\": content})\n",
    "    reply = content\n",
    "    return reply, messages\n",
    "\n",
    "def app(IP, Port=5010, model_engine='gpt-3.5-turbo'):\n",
    "    \"\"\"\n",
    "    Launch a local web server of chatGPT\n",
    "    \n",
    "    ES:\n",
    "        --IP your server IP \n",
    "        --Port the port you want to use | default=5010\n",
    "        --model_engine the LLM engine you use from openAI | default='gpt-3.5-turbo'\n",
    "    \"\"\"\n",
    "    chatGPT = gr.Blocks(title='chatGPT by D.X')\n",
    "    with chatGPT:\n",
    "        gr.Markdown(\"<h1><center>ChatGPT of gpt-3.5-turbo</center></h1>\")\n",
    "        chatbot = gr.Chatbot()\n",
    "        msg = gr.Textbox(placeholder='Input the things you want chatGPT to help you.')\n",
    "        messages = gr.State(value=[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}])\n",
    "        clear = gr.Button(\"Clear\") \n",
    "\n",
    "        def user(user_message, messages, history):\n",
    "            messages.append({\"role\": \"user\", \"content\": user_message})\n",
    "            return \"\", history + [[user_message, None]]\n",
    "\n",
    "        def bot(user_message, messages, history):\n",
    "            reply, messages = generate_response(model_engine, messages)\n",
    "            history[-1][1] = reply\n",
    "            return history\n",
    "\n",
    "        msg.submit(user, [msg, messages, chatbot], [msg, chatbot], queue=False).then(\n",
    "            bot, [msg, messages, chatbot], chatbot\n",
    "        )\n",
    "        clear.click(lambda: None, None, chatbot, queue=False)\n",
    "    \n",
    "    chatGPT.launch(server_name=IP, server_port=Port, share=True, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "if __name__ == \"__main__\":\n",
    "    fire.Fire(app)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
